# Web Scraper Service

**Requirements Specification**

---

# 1. Technical Requirements

These define **what must be built and how it must behave**.

---

## 1.1 Architecture & Communication

* The service **must be a standalone internal microservice**
* Communication **must use gRPC**
* gRPC **must support streaming for progress updates**
* All internal services **must access scraping through this service**
* Direct web scraping from other services is **strictly forbidden**

### 1.1.1 Call chain (End user → UI → Gateway → Scraper Service)

- **End user** interacts with a platform UI (e.g., a CYON UI) and can only **trigger** a scraping/discovery workflow.
- **Gateway (service client)** prepares the full payload (document list / config / discovery options) and calls this gRPC service.
- **Scraper service** executes the job deterministically and streams progress back to the gateway.

Notes:
- Presets/configurations are owned by the **gateway** (dynamic presets). This service must not rely on hardcoded internal presets.
- Phase 3 will extend the gateway to accept end-user inputs (e.g., source URL + prompt) and translate them into a concrete payload for this service.

---

## 1.2 Supported Capabilities

### Mandatory Capabilities (Phase 1)

* Configured scraping with JSON structure
* Content noise filtering (headers, footers, navigation)
* Document generation (PDF, DOCX)
* Image handling (embed, link, or both)
* MinIO storage with folder hierarchy
* Source URL attribution

### Mandatory Capabilities (Phase 2)

* Autonomous site discovery
* Deterministic crawling within a `base_url` scope (no LLM decisions for which pages to visit)
* Discovery-time quality assessment and filtering (configurable thresholds; system caps apply)
* Generic hierarchy inference (link-graph + URL-path clustering heuristics; not site-hardcoded)
* LLM-powered **noise filtering suggestions** only (e.g., propose `noise_selectors` for header/footer/nav/ads)
* Approval workflow support (discover → review → approve and scrape)

### Optional (Not in Phase 1)

* Multi-site batch processing
* Real-time content monitoring
* Advanced image OCR
* Content change detection

---

## 1.3 API Behavior

### Input Requirements (Phase 1)

The API **must accept** (from the gateway/service client):

**Scraping Configuration**

* `base_url` - Target documentation site
* `structure` - JSON hierarchy of categories and documents
* `options` - Processing preferences

**Processing Options**

* `output_format` - PDF, DOCX, or both
* `image_handling` - embed, link_only, or both
* `noise_selectors` - CSS selectors to remove
* `include_metadata` - Boolean for metadata generation

### Input Requirements (Phase 2)

The API **must accept** (from the gateway/service client):

**Discovery Configuration**

* `base_url` - Site to analyze autonomously
* `discovery_options` - Quality and depth settings
* `scraping_options` - Same as Phase 1 options

Rules:

* User input **must never override** system constraints
* System limits always take priority
* Input validation is mandatory

---

### Output Modes

#### Configured Scraping Mode

* Process documents according to JSON structure
* Return completion status with metadata
* Include processing statistics
* List any errors encountered

#### Autonomous Discovery Mode

* Stream progress updates during discovery
* Return discovered structure for validation
* Process documents after approval
* Include LLM analysis results

---

## 1.4 Content Processing

### Noise Filtering

* Must remove: navigation, headers, footers, sidebars
* Must preserve: main content, code examples, tables
* Must be configurable via CSS selectors
* Must handle dynamic content properly

### Document Generation

* PDF generation with proper formatting
* DOCX generation with structured content
* Image handling per configuration
* Source URL inclusion in all documents

### Storage Structure

* MinIO bucket with folder hierarchy
* Category-based organization
* Metadata files for each document
* Consistent naming conventions

---

## 1.5 LLM Runtime (Phase 2)

* LLM runtime **must be open source (Ollama or vLLM)**
* Runtime must run **locally or inside same network**
* Initial supported model:

    * **Qwen 2.5 3B** or equivalent open source
* Runtime communication **must not be exposed publicly**
* Models must be capable of on-premises deployment

---

## 1.6 Service Stack

* Backend framework: **FastAPI**
* Protocol layer: **gRPC**
* Language: **Python**
* Scraping: **Playwright + BeautifulSoup**
* Document generation: **WeasyPrint + python-docx**
* Storage: **MinIO + PostgreSQL(sqlalchemy 2.0)**
* Containerization: **Docker**
* Service must be deployable without external dependencies

---

## 1.7 Database Requirements

### Database

* PostgreSQL(sqlalchemy 2.0) or compatible NoSQL
* Used only for:

    * Job tracking
    * Document metadata
    * Processing statistics
    * Error logging

### Logged Data (Mandatory)

* Job ID and configuration
* Source URLs and processing status
* Document metadata (size, format, quality)
* Processing time and performance metrics
* Error details and retry counts
* LLM analysis results (Phase 2)

### System Configuration (Stored)

* Default noise selectors
* Output format preferences
* Rate limiting settings
* Quality thresholds
* Runtime configuration

---

## 1.8 Observability

* Structured logging required
* Logs must be queryable
* Processing progress must be trackable
* Performance metrics must be measurable
* Error rates must be monitorable
* Storage usage must be observable

---

## 1.9 Security Constraints

* Service is **internal-only**
* No public endpoints
* No secrets in code or logs
* Rate limiting for external sites
* Respect robots.txt
* gRPC clients must be authenticated

---

## 1.10 Performance & Reliability

* Service must handle concurrent scraping jobs
* Blocking operations are forbidden in request paths
* Scraping must be cancellable
* Timeouts must fail gracefully
* Service must not crash on malformed input
* Must respect website rate limits

---

## 1.11 Explicit Non-Goals (Phase 1)

* No UI for browsing
* No public web interface
* No real-time monitoring dashboards
* No content transformation beyond format conversion
* No automatic scheduling (manual triggers only)

---

# 2. Non-Technical Requirements

These define **constraints, expectations, and operating principles**.

---

## 2.1 Ownership & Responsibility

* Web Scraper Service is owned by **Platform / Infrastructure**
* Application teams are **clients** (gRPC users), not owners
* Scraping configuration changes do **not require client changes**
* Document format changes must not break clients

---

## 2.2 Consistency & Standardization

* All services must use **same scraping access pattern**
* Document formats must be consistent across services
* Storage structure must be standardized
* Error handling must be uniform
* Logging format must be standardized

---

## 2.3 Cost & Resource Control

* Storage usage must be observable
* High-usage services must be identifiable
* System limits exist to prevent abuse
* Processing time must be trackable
* Resource control is a **first-class concern**

---

## 2.4 Change Management

* Document format changes must not break clients
* API contracts must remain backward compatible
* Breaking changes require versioning
* LLM model changes must not affect Phase 1

---

## 2.5 Scalability Expectations

* Designed to support:
    * Multiple internal services
    * Multiple document formats
    * Large documentation sites
* Horizontal scaling is expected later
* Architecture must not block future expansion

---

## 2.6 Developer Experience

* Clear gRPC contracts
* Clear configuration patterns
* Predictable error behavior
* Deterministic responses
* Easy local development using Docker

---

## 2.7 Operational Simplicity

* Single entry point for document scraping
* Centralized logging and metrics
* One place to debug scraping issues
* Reduced cognitive load for product teams

---

## 2.8 Compliance & Data Control

* No unauthorized data extraction
* Data residency remains on-prem
* Logs must be auditable
* Source attribution must be maintained
* Respect for website terms of service

---

# 3. Acceptance Criteria (High Level)

The service is considered **acceptable** when:

## Phase 1 Acceptance

* At least one internal service can:
    * Submit scraping configuration
    * Receive processed documents
    * Access documents in MinIO
* Content filtering removes navigation noise
* Documents are generated in specified formats
* Source URLs are properly attributed
* Processing statistics are logged

## Phase 2 Acceptance

* LLM can analyze site structure
* Autonomous discovery works without manual configuration
* Quality assessment filters out low-quality content
* Generated documents maintain 90%+ content fidelity
* Processing handles 100+ pages per job

## General Acceptance

* No service depends directly on Playwright or MinIO
* All scraping goes through this service
* Rate limiting prevents website abuse
* Error handling is deterministic
* Storage structure is consistent and queryable

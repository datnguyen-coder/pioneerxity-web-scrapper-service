# Web Scraper Service

**High-Level Design & Base Requirements**

---

## 1. Background & Motivation

The platform requires a systematic approach to collect and process documentation from various external services for RAG (Retrieval-Augmented Generation) purposes. Currently, there is no centralized way to:

* Extract structured documentation from external service websites
* Filter out irrelevant content (navigation, footers, etc.)
* Store documents in a standardized format for embedding
* Maintain source attribution and metadata
* Scale documentation collection across multiple services

The Web Scraper Service addresses these needs by providing a **two-phase approach**:
1. **Phase 1:** Manual/configurable scraping with structured JSON configuration
2. **Phase 2:** Autonomous **deterministic** discovery (crawl + hierarchy inference), with LLM used only to suggest **noise filtering** selectors

---

## 2. What – What Is the Web Scraper Service?

The **Web Scraper Service** is an **internal gRPC microservice** responsible for:

* Extracting documentation from external service websites
* Filtering out non-essential content (headers, footers, navigation)
* Converting content to standardized formats (PDF, DOCX, etc.)
* Storing processed documents in object storage (MinIO)
* Maintaining source attribution and metadata
* Supporting both manual configuration and autonomous discovery

This service is **infrastructure-level**, designed to support RAG systems across the platform.

### 2.1 Service client vs end user (responsibility split)

- **Service client**: an internal backend gateway (or other internal backend) that prepares the full request payload and calls this gRPC service.
- **End user**: only triggers workflows through the gateway. End users do not call this service directly and do not send raw parameters to it.

---

## 3. Why – Why This Service Exists

Multiple platform services need access to external documentation for:

* RAG embedding and knowledge bases
* API documentation integration
* Competitive analysis and research
* Automated knowledge base updates

Instead of:
* Ad-hoc scraping scripts per service
* Inconsistent document formats
* Manual document collection processes

We introduce a **centralized scraping service** that provides:
* Consistent content filtering and processing
* Standardized output formats
* Automated source attribution
* Configurable extraction rules

---

## 4. How – High-Level Architecture

### 4.1 Phase 1: Manual/Static Scraping

**Execution Flow:**
1. Client sends gRPC request with JSON configuration
2. Service parses configuration and builds site structure
3. For each document URL:
   * Fetch page content
   * Apply noise filtering (remove headers, footers, navigation)
   * Extract main content and images with an url along side it
   * Convert to target format (PDF/DOCX)
   * Store in MinIO with structured folder hierarchy
4. Return completion status with metadata

**Configuration Structure:**
```json
{
  "base_url": "https://service.com/docs",
  "structure": {
    "category1": {
      "subcategory1": ["doc1.html", "doc2.html"],
      "subcategory2": ["doc3.html"]
    },
    "category2": {
      "subcategory1": ["doc4.html"]
    }
  },
  "options": {
    "include_images": true,
    "image_handling": "embed_or_link",
    "output_format": "pdf",
    "noise_selectors": ["header", "footer", ".navigation"] // optional
  }
}
```

### 4.2 Phase 2: Autonomous LLM-Powered Discovery

**Execution Flow:**
1. Gateway/service client provides base URL (e.g., https://ai-sdk.dev/docs)
2. Service crawls deterministically within scope and discovers a documentation hierarchy
3. LLM supports **content/noise filtering suggestions** only (e.g., suggests noise selectors like header/footer/search/ads)
4. Service extracts and processes discovered documents (after approval workflow)
5. Results stored with metadata and a deterministic hierarchy suitable for downstream indexing

Note:
- Presets/configurations are owned by the **gateway** (dynamic presets). This service is preset-agnostic and executes the payload it receives.

---

## 5. Core Requirements

### 5.1 Content Filtering

**Must filter out:**
* Navigation menus and breadcrumbs
* Headers and footers
* Search bars and filters
* Sidebar content
* Advertisement sections
* Social media widgets

**Must preserve:**
* Main documentation content
* Code examples and snippets
* Images and diagrams (configurable)
* Tables and structured data
* Cross-references and links

### 5.2 Document Processing

**Output Formats:**
* PDF (primary)
* DOCX (secondary)
* JSON metadata (always)

**Image Handling Options:**
* Embed images directly in documents
* Include image links as references
* Both embedded and linked versions

**Required Metadata:**
* Source URL
* Extraction timestamp
* Content type
* File size
* Processing duration

### 5.3 Storage Structure

**MinIO Folder Hierarchy:**
```
service-name/
├── category1/
│   ├── subcategory1/
│   │   ├── document1.pdf
│   │   ├── document1.metadata.json
│   │   └── images/
│   └── subcategory2/
└── category2/
```

---

## 6. API Requirements (High Level)

### 6.1 Phase 1: Configured Scraping

**Input Contract:**
```protobuf
message ScrapeRequest {
  string base_url = 1;
  DocumentStructure structure = 2;
  ScrapingOptions options = 3;
}

message DocumentStructure {
  map<string, Category> categories = 1;
}

message Category {
  map<string, DocumentList> subcategories = 1;
  repeated string documents = 2;
}

message ScrapingOptions {
  OutputFormat format = 1;
  ImageHandling image_handling = 2;
  repeated string noise_selectors = 3;
  bool include_metadata = 4;
}
```

**Output Contract:**
```protobuf
message ScrapeResponse {
  bool success = 1;
  repeated ProcessedDocument documents = 2;
  repeated string errors = 3;
  ProcessingStats stats = 4;
}

message ProcessedDocument {
  string minio_path = 1;
  string source_url = 2;
  int64 file_size_bytes = 3;
  string content_type = 4;
  int64 processing_time_ms = 5;
}
```

### 6.2 Phase 2: Autonomous Discovery

**Input Contract:**
```protobuf
message AutonomousScrapeRequest {
  string base_url = 1;
  DiscoveryOptions options = 2;
}

message DiscoveryOptions {
  int32 max_depth = 1;
  repeated string allowed_domains = 2;
  repeated string exclude_patterns = 3;
  QualityThresholds quality_thresholds = 4;
}
```

---

## 7. Technology Stack

### Core Service
* **Framework:** FastAPI
* **Protocol:** gRPC
* **Language:** Python 3.11+

### Web Scraping
* **Primary:** Playwright (for dynamic content)
* **Fallback:** BeautifulSoup4 (for static content)
* **HTML Processing:** lxml, html5lib

### Document Generation
* **PDF:** WeasyPrint or ReportLab
* **DOCX:** python-docx
* **Image Processing:** Pillow (PIL)

### Storage
* **Object Storage:** MinIO
* **Metadata:** PostgreSQL + sqlalchemy 2.0 or any kind of noSQL

### LLM Integration (Phase 2)
* **LLM Runtime:** Ollama or vLLM (on-premises)
* **Initial Model:** Qwen 2.5 3B or equivalent open source model
* **Model Requirements:** Must be open source and capable of running on-premises

---

## 8. Database Responsibilities

### PostgreSQL Schema (High Level)

**Documents Table:**
* Document metadata
* Source URLs and timestamps
* Processing status and errors
* File paths and sizes

**Scraping Jobs Table:**
* Job configuration
* Status tracking
* Performance metrics
* Error logs

**Site Structures Table:**
* Discovered site hierarchies
* LLM-generated categorizations
* Quality scores

---

## 9. Implementation Phases

### Phase 1: Manual Scraping (MVP)
1. Basic web scraping with Playwright
2. Noise filtering with configurable selectors
3. PDF generation and MinIO storage
4. gRPC API implementation
5. Basic metadata tracking

### Phase 2: Autonomous Discovery
1. Deterministic crawl within a `base_url` scope
2. Generic structure discovery (link-graph + URL-path clustering heuristics)
3. Discovery-time quality assessment and filtering
4. LLM integration for **noise filtering suggestions** (not page/topic decisions)
5. Advanced metadata generation

---

## 10. Quality Assurance

### Content Quality Metrics
* Text-to-noise ratio
* Image relevance scoring
* Link preservation rate
* Format consistency

### Performance Metrics
* Pages processed per minute
* Average processing time
* Error rates by domain
* Storage efficiency

### Reliability Features
* Retry mechanisms for failed requests
* Rate limiting and politeness policies
* Error categorization and reporting
* Incremental processing support

---

## 11. Security Considerations

### Access Control
* Internal service only
* gRPC authentication
* Rate limiting per client

### Data Handling
* Sanitize extracted content
* Validate file formats
* Secure MinIO credentials
* Audit logging for all operations

### External Interaction
* Respect robots.txt
* Implement rate limiting
* User-Agent rotation
* Request timeout management

---

## 12. Scope of This Phase

**Included:**
* High-level architecture
* API contracts
* Technology stack selection
* Phase 1 requirements definition
* Phase 2 roadmap

**Out of scope for this phase:**
* Detailed database schema design
* Specific LLM prompts for Phase 2
* Performance optimization details
* Error handling specifications
* Deployment configurations

---

## 13. Next Steps

1. Set up basic FastAPI + gRPC service structure
2. Implement Playwright-based scraping engine
3. Create noise filtering system
4. Implement PDF generation and MinIO storage
5. Build Phase 1 gRPC API
6. Test with sample documentation sites
7. Begin Phase 2 LLM integration design

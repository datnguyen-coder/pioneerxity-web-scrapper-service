# Web Scraper Service

**Developer Guidelines (Do / Don't / Patterns)**

**Audience:** Backend engineers working on Web Scraper Service
**Scope:** Service implementation only (not client services)

---

## 0. Guiding Principles (Read First)

* This service is **infrastructure**, not product logic
* Reliability > speed
* Consistency > cleverness
* No shortcuts that leak into client services
* If a decision affects multiple services → it belongs here

---

## 1. DO (Mandatory Practices)

### 1.1 Architecture & Layering

✅ DO enforce strict layering:

```
gRPC Layer (Controller)
        ↓
Service Layer (Business Logic)
        ↓
Scraping Layer (Playwright/BeautifulSoup)
        ↓
Processing Layer (Document Generation)
        ↓
Storage Layer (MinIO/PostgreSQL)
```

Rules:

* Controllers only handle gRPC I/O
* Services contain orchestration logic
* Scraping layer handles web interactions
* Processing layer handles document creation
* Storage layer handles data persistence
* No layer skipping allowed

---

### 1.2 gRPC & Streaming

✅ DO:

* Use **async** gRPC handlers
* Support streaming for progress updates
* Preserve job status exactly
* Handle client cancellation immediately
* Close streams explicitly on completion or error

Streaming must:

* Yield progress updates incrementally
* Never buffer full job status
* Stop instantly on cancellation
* Include error details in stream

---

### 1.3 Content Filtering

✅ DO:

* Apply noise filters before content extraction
* Use configurable CSS selectors
* Preserve main content structure
* Handle dynamic content properly
* Validate content quality before processing

Content filtering must:

* Remove navigation, headers, footers
* Preserve articles, documentation, code
* Handle both static and dynamic content
* Be configurable per job

---

### 1.4 Document Processing

✅ DO:

* Generate documents in requested format
* Include source URL attribution
* Handle images per configuration
* Maintain consistent formatting
* Validate output before storage

Document generation must:

* Support PDF and DOCX formats
* Embed or link images correctly
* Preserve text formatting
* Include metadata when requested

---

### 1.5 Storage & Organization

✅ DO:

* Use MinIO for document storage
* Follow folder hierarchy from configuration
* Store metadata in PostgreSQL
* Use consistent naming conventions
* Track processing status

Storage structure must:

* Reflect input JSON hierarchy
* Include category/subcategory paths
* Store metadata alongside documents
* Be queryable by job ID

---

### 1.6 Logging & Observability

✅ DO log:

* Job ID and configuration
* Processing progress and status
* Document metadata and quality
* Processing time and performance
* Error details and retry counts

Logs must be:

* Structured (JSON)
* Non-blocking
* Queryable
* Include correlation IDs

---

### 1.7 Error Handling

✅ DO:

* Map internal errors → domain errors → gRPC errors
* Return deterministic error codes
* Include job ID in all errors
* Fail fast on invalid input
* Retry transient failures appropriately

Error handling must:

* Categorize errors by type
* Provide actionable error messages
* Include retry recommendations
* Never expose internal stack traces

---

### 1.8 Rate Limiting & Politeness

✅ DO:

* Respect robots.txt
* Implement rate limiting per domain
* Use appropriate delays between requests
* Rotate user agents when needed
* Handle rate limit responses gracefully

Rate limiting must:

* Be configurable per domain
* Prevent website abuse
* Allow concurrent processing of different sites
* Track rate limit violations

---

### 1.9 Docker & Local Dev

✅ DO:

* Support `docker-compose up` locally
* Make MinIO and PostgreSQL reachable
* Provide `.env.example`
* Ensure service starts without manual steps
* Include test data fixtures

---

## 2. DON'T (Strictly Forbidden)

### 2.1 Architecture Violations

❌ DON'T:

* Call Playwright directly from controllers
* Let clients know MinIO exists
* Mix storage logic into scraping layer
* Access DB directly from controllers
* Skip content filtering

---

### 2.2 Content & Data Safety

❌ DON'T:

* Store raw HTML content unnecessarily
* Extract sensitive information
* Ignore robots.txt
* Process content without validation
* Allow unlimited concurrent requests to same domain

---

### 2.3 Performance Killers

❌ DON'T:

* Use blocking I/O in request paths
* Buffer entire documents in memory
* Retry blindly on network errors
* Spawn unmanaged background tasks
* Process documents synchronously

---

### 2.4 API & Contract Stability

❌ DON'T:

* Change gRPC contracts without versioning
* Add optional behavior without defaults
* Return inconsistent response shapes
* Leak internal exceptions to clients
* Break existing folder structures

---

### 2.5 Operational Anti-Patterns

❌ DON'T:

* Crash service on malformed input
* Assume single document format forever
* Hardcode storage paths in business logic
* Introduce silent failures
* Ignore rate limiting

---

## 3. REQUIRED PATTERNS

These patterns **must be followed**.

---

### 3.1 Job Processing Pattern

Every scraping job must follow:

1. Validate input configuration
2. Apply system constraints
3. Initialize scraping resources
4. Process documents according to structure
5. Generate documents in requested formats
6. Store with proper hierarchy
7. Log metrics and status
8. Return completion response

No step may be skipped.

---

### 3.2 Content Filtering Pattern

Rules:

* Apply noise filters first
* Extract main content second
* Validate content quality third
* Process images fourth

Never process unfiltered content.

---

### 3.3 Document Generation Pattern

Rules:

* Generate in requested format
* Include source attribution
* Handle images per configuration
* Validate output before storage
* Store with consistent naming

---

### 3.4 Storage Pattern

MinIO + PostgreSQL pattern:

* Documents → MinIO (object storage)
* Metadata → PostgreSQL (structured data)
* Jobs → PostgreSQL (tracking)
* Errors → PostgreSQL (auditing)

Never store documents in PostgreSQL.

---

### 3.5 Error Mapping Pattern

Internal error → Domain error → gRPC status

Examples:

* Invalid URL → `INVALID_URL`
* Network timeout → `NETWORK_TIMEOUT`
* Storage failure → `STORAGE_ERROR`
* Content filtering error → `CONTENT_PROCESSING_ERROR`
* Rate limit exceeded → `RATE_LIMIT_EXCEEDED`

Never expose raw stack traces.

---

### 3.6 Configuration Pattern

* Read-only during job execution
* Centralized access
* Cached where appropriate
* Reload-safe
* Validated on startup

---

## 4. PHASE 2 LLM INTEGRATION PATTERNS

### 4.1 LLM Runtime Pattern

Purpose: isolate LLM runtimes and enable switching.

```
LLMDiscoveryService
  └── LLMRuntime (interface)
        ├── OllamaAdapter
        ├── vLLMAdapter
        └── (future adapters)
```

Rules:

* Service talks only to runtime interface
* No runtime-specific logic outside adapters
* Adapters return normalized output
* Runtime type is configurable

---

### 4.2 Site Discovery Pattern

Every autonomous discovery must follow:

1. Crawl initial pages from base URL
2. Analyze structure with LLM
3. Validate discovered hierarchy
4. Filter by quality thresholds
5. Present structure for approval
6. Process approved documents

---

### 4.3 Quality Assessment Pattern

LLM must assess:

* Content relevance and quality
* Structure coherence
* Navigation patterns
* Document completeness

Quality scores must be:

* Stored with document metadata
* Used for filtering decisions
* Configurable thresholds
* Logged for analysis

---

## 5. Definition of Done (For This Service)

A feature is **DONE** only if:

* gRPC contract updated (if applicable)
* Content filtering tested with various sites
* Document generation works for all formats
* Storage structure is consistent
* Error handling is deterministic
* Rate limiting is functional
* Code follows all rules in this document
* Phase 2: LLM integration works end-to-end

---

## 6. When to Escalate (Ask Before Proceeding)

Ask Tech Lead **before**:

* Changing gRPC contracts
* Adding new document formats
* Modifying storage structure
* Changing LLM runtime integration
* Adding new content filtering rules
* Modifying rate limiting logic

---

## 7. Final Rule

> If a client service needs to know *how* web scraping works,
> then this service has already failed.
